{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91acd011-f98b-45f9-8943-4acf3230860a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9ca76-8a3d-4a11-8b78-984e5db4c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"As dawn broke over the sleepy village, a gentle breeze stirred the leaves, whispering secrets to the awakening world. The scent of dew-kissed grass hung in the air, mingling with the aroma of freshly brewed coffee wafting from the local caf√©. Birds chirped joyfully, heralding the start of a new day, while the sun peeked over the horizon, casting a golden glow upon the cobblestone streets. In the distance, the silhouette of a lone figure could be seen, ambling along the winding path with purposeful strides. Each step seemed to echo the rhythm of the village, a harmonious melody of life unfolding with every passing moment. As the day unfolded, the village embraced the promise of adventure and possibility, basking in the beauty of the present moment.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ff3b2-5ffb-47d4-8129-07df5d4e125f",
   "metadata": {},
   "source": [
    "## Using Built In Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834aa81-f82f-4a94-83af-7c298be4f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = text.split()\n",
    "sent = text.split('.')\n",
    "print(\"Word Tokenization: \",word)\n",
    "print(\"Sentence Tokenization: \",sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f89b87-96b7-4205-8709-0cc20c34d06f",
   "metadata": {},
   "source": [
    "## Using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86210e6c-7c45-4855-8174-a2eb6c0bda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40878b-2637-4629-9850-602d7fcc4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = re.findall('[\\w]+', text)\n",
    "sent = re.compile('[.]').split(text)\n",
    "print(\"Word Tokenization: \",word)\n",
    "print(\"Sentence Tokenization: \",sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa97300-9efa-4fd6-b5d5-b7d7fe04fec3",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67657395-2cbb-40db-8169-6a369ab9b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762629a2-488a-4fd5-8d09-1a31f10c89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addb1cd-5b69-4bcb-900d-c0033cdbeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "word = word_tokenize(text)\n",
    "sent = sent_tokenize(text)\n",
    "print(\"Word Tokenization: \",word)\n",
    "print(\"Sentence Tokenization: \",sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f94b49-57a9-4a43-a013-591ae530b3de",
   "metadata": {},
   "source": [
    "## Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bb2b3-0233-457d-a03c-643c0f87af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "word = []\n",
    "for t in doc:\n",
    "    word.append(t.text)\n",
    "sent = []\n",
    "for t in doc.sents:\n",
    "    sent.append(t.text)\n",
    "\n",
    "print(\"Word Tokenization: \",word)\n",
    "print(\"Sentence Tokenization: \",sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18523a6-baa4-4312-b80e-4070dbf35162",
   "metadata": {},
   "source": [
    "## Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67061817-24f2-4afc-9025-69e44cdc77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2ac1c-6682-4e05-90e1-357d2242f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = text_to_word_sequence(text)\n",
    "sent = text_to_word_sequence(text, split='.')\n",
    "print(\"Word Tokenization: \",word)\n",
    "print(\"Sentence Tokenization: \",sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63665767-bcbe-44bc-98ed-8a595922c132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
